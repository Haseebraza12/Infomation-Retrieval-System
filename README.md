# ğŸ§  Cortex IR - Advanced News Article Search Engine

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A state-of-the-art hybrid information retrieval system for news articles, combining BM25, ColBERT, neural reranking, and intelligent post-processing for superior search performance.

## ğŸŒŸ Features

- **ğŸ” Hybrid Search**: Combines sparse (BM25) and dense (ColBERT) retrieval
- **ğŸ§  Neural Reranking**: Cross-encoder model for improved relevance
- **ğŸ¯ Query Intelligence**: Automatic classification, expansion, and spell correction
- **ğŸŒˆ Diversity Aware**: MMR algorithm for diverse results
- **â° Temporal Intelligence**: Context-aware temporal boosting
- **ğŸ”— Entity Deduplication**: Remove duplicates based on named entities
- **ğŸ“Š Topic Clustering**: Organize results by themes
- **ğŸ¨ Modern UI**: Beautiful Gradio interface with animations and visualizations
- **âš¡ Fast Performance**: ~10-15ms query latency on CPU
- **ğŸš€ Auto-Setup**: Automatically preprocesses and indexes data on first run

## ğŸ“ Project Structure

```
implementation/
â”œâ”€â”€ config.py                 # Configuration management
â”œâ”€â”€ utils.py                  # Utility functions
â”œâ”€â”€ preprocessing.py          # Stage 0: Article preprocessing
â”œâ”€â”€ indexing.py              # Stage 0: Index construction
â”œâ”€â”€ query_processor.py        # Stage 1: Query processing
â”œâ”€â”€ retrieval.py             # Stage 1: Hybrid retrieval
â”œâ”€â”€ reranker.py              # Stage 2: Neural reranking
â”œâ”€â”€ post_processor.py        # Stage 3: Post-processing
â”œâ”€â”€ main.py                  # Complete pipeline orchestration
â”œâ”€â”€ gradio_app.py            # Gradio web interface
â”œâ”€â”€ evaluation.py            # Evaluation metrics
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env.example            # Environment variables template
â””â”€â”€ README.md               # This file
```

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 0: Preprocessing & Indexing (Offline)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ spaCy NLP pipeline (tokenization, lemmatization, NER)     â”‚
â”‚  â€¢ BM25+ sparse index (bm25s library)                        â”‚
â”‚  â€¢ ColBERTv2 dense index (RAGatouille)                       â”‚
â”‚  â€¢ SQLite metadata store                                      â”‚
â”‚  â€¢ BERTopic clustering                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 1: Hybrid Retrieval (~60-80ms)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Query classification (breaking/historical/factual/        â”‚
â”‚    analytical)                                                â”‚
â”‚  â€¢ Parallel BM25 + ColBERT retrieval                         â”‚
â”‚  â€¢ Reciprocal Rank Fusion (RRF)                              â”‚
â”‚  â€¢ Returns top 100 candidates                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 2: Neural Reranking (~200-280ms)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Cross-encoder (ms-marco-MiniLM-L-6-v2)                   â”‚
â”‚  â€¢ Batch processing for efficiency                           â”‚
â”‚  â€¢ Ensemble scoring with multiple signals                    â”‚
â”‚  â€¢ Returns top 50 candidates                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 3: Post-Processing (~40ms)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Diversity reranking (MMR)                                 â”‚
â”‚  â€¢ Temporal intelligence boosting                            â”‚
â”‚  â€¢ Entity-based deduplication                                â”‚
â”‚  â€¢ Category balancing                                        â”‚
â”‚  â€¢ Topic clustering for presentation                         â”‚
â”‚  â€¢ Returns final 20 results                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- 16GB RAM recommended
- 5GB disk space for indices

### Installation

1. **Clone the repository** (or navigate to project directory):

```bash
cd implementation
```

2. **Create virtual environment**:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**:

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

4. **Download spaCy model**:

```bash
python -m spacy download en_core_web_sm
```

5. **Configure environment** (optional):

```bash
cp .env.example .env
# Edit .env with your preferred settings
```

## ğŸš€ Quick Start

**Just run one command:**

```bash
python app.py
```

Or directly:

```bash
python gradio_app.py
```

The system will automatically:
- âœ… Check if data is preprocessed (if not, preprocess Articles.csv)
- âœ… Check if indices are built (if not, build BM25 and metadata indices)
- âœ… Launch the Gradio web interface at `http://localhost:7860`

**First run takes ~3-5 minutes for preprocessing and indexing. Subsequent runs start instantly.**

### Manual Component Usage

If you need to run components separately:

```bash
# Step 1: Preprocess articles
python preprocessing.py

# Step 2: Build indices (BM25, metadata)
python indexing.py

# Step 3: Launch UI
python gradio_app.py
```

### Command-Line Usage

```python
from main import CortexIRPipeline

# Initialize pipeline
pipeline = CortexIRPipeline()

# Execute search
results = pipeline.search(
    query="latest sports news",
    top_k=10,
    enable_reranking=True,
    enable_post_processing=True
)

# Display results
for i, doc in enumerate(results['results'], 1):
    print(f"{i}. {doc['title']}")
    print(f"   Score: {doc.get('ensemble_score', 0):.4f}")
    print(f"   {doc['snippet']}\n")
```

## ğŸ“Š Configuration

Edit `config.py` or create a `.env` file to customize:

### Key Parameters

```python
# BM25 Parameters
BM25_K1 = 1.5              # Term frequency saturation
BM25_B = 0.75              # Length normalization
TITLE_BOOST = 3.0          # Title field weight
ENTITY_BOOST = 2.0         # Entity weight

# Retrieval
TOP_K_SPARSE = 100         # BM25 candidates
TOP_K_DENSE = 100          # ColBERT candidates
RRF_K = 60                 # RRF fusion parameter

# Reranking
RERANK_BATCH_SIZE = 32     # Batch size for cross-encoder
RERANK_TOP_K = 50          # Documents to rerank

# Post-processing
FINAL_TOP_K = 20           # Final results
DIVERSITY_LAMBDA_FACTUAL = 0.9      # Relevance weight
DIVERSITY_LAMBDA_EXPLORATORY = 0.6   # Balanced weight
```

## ğŸ§ª Evaluation

Evaluate system performance:

```python
from main import CortexIRPipeline
from evaluation import SystemEvaluator

# Initialize
pipeline = CortexIRPipeline()
evaluator = SystemEvaluator(pipeline)

# Define test queries and relevance judgments
test_queries = ["sports news", "economic trends"]
relevant_docs = [[1, 5, 10], [2, 8, 15]]

# Evaluate
evaluation = evaluator.evaluate_queries(test_queries, relevant_docs)

# Print report
evaluator.print_evaluation_report(evaluation)
```

### Metrics Calculated

- **MAP** (Mean Average Precision)
- **MRR** (Mean Reciprocal Rank)
- **Precision@K** (K = 5, 10, 20)
- **Recall@K** (K = 5, 10, 20)
- **NDCG@K** (Normalized Discounted Cumulative Gain)
- **Response Time** (Average and standard deviation)

## ğŸ¨ Web Interface Features

### Search Tab
- ğŸ” Intelligent search with auto-correction
- ğŸ›ï¸ Configurable result count
- âš™ï¸ Toggle reranking and post-processing
- ğŸ“Š Real-time performance metrics
- ğŸ“ˆ Interactive visualizations
- ğŸ·ï¸ Category and timeline distributions
- ğŸ’¡ Example queries

### Analytics Tab
- ğŸ“‰ Search history statistics
- â±ï¸ Average response times
- ğŸ¯ Query type distribution
- ğŸ“Š Usage patterns

### About Tab
- ğŸ“– System documentation
- ğŸ—ï¸ Architecture overview
- ğŸ› ï¸ Technology stack
- ğŸ“ˆ Performance specifications

## ğŸ”§ Module Details

### preprocessing.py
- **ArticlePreprocessor**: Tokenization, lemmatization, NER
- **QueryPreprocessor**: Query processing and entity extraction
- Uses spaCy for NLP tasks
- Filters entities by confidence (threshold: 0.85)

### indexing.py
- **BM25Indexer**: Fast sparse index with bm25s
- **ColBERTIndexer**: Dense retrieval with RAGatouille
- **MetadataIndexer**: SQLite database with B-tree indices
- **TopicIndexer**: BERTopic for document clustering

### query_processor.py
- **QueryClassifier**: Classifies queries into 4 types
- **QueryExpander**: PRF-based query expansion
- **SpellCorrector**: Common spelling corrections
- **QueryProcessor**: Complete query pipeline

### retrieval.py
- **BM25Retriever**: Sparse retrieval
- **ColBERTRetriever**: Dense retrieval
- **ReciprocalRankFusion**: Score fusion
- **HybridRetriever**: Parallel hybrid search

### reranker.py
- **NeuralReranker**: Cross-encoder reranking
- **EnsembleReranker**: Multi-signal ensemble
- Batch processing for efficiency
- Early stopping optimization

### post_processor.py
- **DiversityReranker**: MMR for diversity
- **TemporalProcessor**: Time-aware boosting
- **EntityDeduplicator**: Duplicate removal
- **CategoryBalancer**: Cross-category balance
- **ResultClusterer**: Topic-based clustering

### main.py
- **CortexIRPipeline**: Complete pipeline orchestration
- Timing and logging
- Batch search support
- Error handling

### gradio_app.py
- **CortexGradioApp**: Modern web interface
- Animated UI components
- Interactive visualizations
- Search analytics dashboard

## ğŸ“ˆ Performance

Typical performance on Core i5 6th Gen + 16GB RAM:

| Stage | Time (CPU) |
|-------|-----------|
| Query Processing | ~10ms |
| Hybrid Retrieval | ~60-80ms |
| Neural Reranking | ~200-280ms |
| Post-Processing | ~40ms |
| **Total** | **~300-400ms** |

## ğŸ§° Technology Stack

### Core IR
- **bm25s**: Fast BM25 implementation
- **RAGatouille**: ColBERTv2 wrapper
- **sentence-transformers**: Embeddings & cross-encoder

### NLP & ML
- **spaCy**: NLP pipeline
- **transformers**: Pre-trained models
- **BERTopic**: Topic modeling
- **PyTorch**: Deep learning

### Storage & Data
- **pandas**: Data manipulation
- **numpy**: Numerical computing
- **SQLite**: Metadata storage

### UI & Visualization
- **Gradio**: Web interface
- **Plotly**: Interactive charts
- **Matplotlib/Seaborn**: Static plots

## ğŸ› Troubleshooting

### Indices Not Found
```bash
# Rebuild indices
python preprocessing.py
python indexing.py
```

### spaCy Model Missing
```bash
python -m spacy download en_core_web_sm
```

### Out of Memory
- Reduce batch size in config: `RERANK_BATCH_SIZE = 16`
- Disable ColBERT: Set `TOP_K_DENSE = 0`
- Reduce candidates: `TOP_K_SPARSE = 50`

### Slow Performance
- Disable reranking for faster results
- Use smaller cross-encoder model
- Reduce number of post-processing steps

## ğŸ“ Example Queries

**Breaking News Queries:**
- "latest COVID updates"
- "recent sports results"
- "breaking business news today"

**Historical Queries:**
- "causes of 2008 financial crisis"
- "history of Olympic games"
- "origin of economic recession"

**Factual Queries:**
- "who won the super bowl?"
- "what is inflation rate?"
- "which team won championship?"

**Analytical Queries:**
- "impact of inflation on economy"
- "trends in sports performance"
- "effects of market volatility"

## ğŸ¤ Contributing

This is an academic project. For improvements:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

MIT License - feel free to use for academic purposes.

## ğŸ‘¨â€ğŸ’» Author

Built as part of CS516 Information Retrieval Assignment , by Haseeb ur rehman

## ğŸ™ Acknowledgments

- **spaCy** for NLP capabilities
- **Hugging Face** for transformer models
- **RAGatouille** for simplified ColBERT integration
- **Gradio** for rapid UI development

---

**Built with â¤ï¸ for advanced information retrieval**
